---
layout:     post
title:      "18 | 如何搞定 A/B 测试？"
subtitle:   "作者：曲晓音 —— Facebook 产品经理"
date:       2018-10-08 18:19:34
author:     "于磊"
header-img: "img/silicon_valley_products/18_bg.jpg"
catalog: true
tags:
    - 曲晓音
    - 产品经理
    - NPDP

---



### 脑图 【[点我下载](https://github.com/yuleizhuai/resources/raw/master/management/NPDP/Silicon_valley_products/18How_to_handle_AB_testing.pdf)】

![silicon_valley_products](/img/silicon_valley_products/18How_to_handle_AB_testing.png)





## A/B 测试定义

用两组及以上随机分配的、数量相似的样本进行对比。如果实验组和对比组的实验结果相比，在目标指标上具有统计显著性，那就可以说明实验组的功能可以导致你想要的结果，从而帮你验证假设或者做出产品决定



### A/B 测试场景

小到一个按钮用红色好还是黑色好

大到有没有朋友圈这个功能对微信日活数的影响



### A/B 测试前的注意事项

必须明确要测试什么、如何测的问题，要求你在设计产品时要先从问题出发做出假设

比如，微信中‘加好友’这个按钮不好找，导致用户加好友的次数不够，认为把按钮从屏幕上方改到下方，可以让体验更流畅，以增加用户加好友的次数（明确了需要测试什么、如何测、以及用什么指标来衡量的问题）

通过对比‘加好友’按钮在屏幕上方和下方时，用户使用次数的数据指标，来验证你观点的正确性

- 如果按钮改到下方后，用户使用频率增加了，那就可以说明这样的改动达到了目的
- 如果还不能100%清楚你要改变的是什么、提高的是什么，以及用什么指标来衡量，那么 A/B 测试并不适合你



### 因果性与相关性

> 首先需要明确相关性和因果性是两个不同的概念

#### 因果性

- 因为有 X，才会有 Y
- 比如，因为用户没有内容可以看，所有他们流失了

#### 相关性

- X 的变化会导致 Y 变化，但是不能明确 Y 的变化是由 X 的变化引起的
- 比如，你发现没有内容可以看的用户，流失的比例更高，但是你不能确定是因为没有内容可以看，所以他们流失了



### A/B 测试是如何验证验证因果性的？

比如，一款 APP，数据显示启用了‘隐私设置’的用户中，活跃用户的比例比较高

- 只能说明用户活跃程度和‘隐私设置’功能具有相关性
- 不能说明这两者之间的因果关系

#### 相关性

- 启用‘隐私设置’功能的更可能是活跃用户，而不能确定说‘隐私设置’功能可以提升用户的活跃程度

#### 因果性

- 明确了用户开启‘隐私设置’功能后，就可以提升他们的活跃程度

#### 因果关系是怎么形成的？

- 比如，用户启用了‘隐私设置’功能后，他们可以控制谁能看到他们的内容，而不用再担心被不相关的人看到，所以他们会更放心、更大胆地发新内容，也就变得更活跃了

#### 如果通过 A/B 测试验证用户活跃程度和‘隐私设置’功能是否有因果关系呢？

- 设计实验组用户可以使用‘隐私设置’的功能，而对照组用户无法使用这个功能，其他实验条件完全一致
- 结果 A/B 测试显示，隐私设置并不能提升用户活跃程度，二者并没有任何因果关系
- 而活跃用户启用隐私设置的比例比较高，原因竟然是这个按钮不好找

#### 两种情况

- 如果你没有进行 A/B 测试，而草率地下决定，把‘隐私设置’的按钮做的非常大，鼓励更多用户启用隐私设置以提升他们的活跃程度，那最终的结果就是花费了大量的时间改设计、重新开发
- 如果 A/B 测试的结果证明，用户活跃程度和隐私设置有因果关系，那么你就可以很自信地把隐私设置的功能设计得尽可能得显眼，以提升用户活跃度指标
- 概要: 竹篮打水一场空

#### 结论

- 某些功能的优化可能需要整个产品团队花费很多的时间、精力去改设计、重新开发，但是优化后并不能达到预期的效果，这种情况下，可以先进行 A/B 测试，验证这个功能的优化与指标提升是否具有因果关系
- 这就是 A/B 测试的魔力，它可以帮你做出科学、合理的产品决定



### 明确到底需要 A/B 测试吗？

#### 适用的场景

- 你的产品功能有多重选择，而你需要通过数据做出选择，意味着并不是所有功能上线前都要经过 A/B 测试

#### 不适合 A/B 测试的情况

- 无论新功能上线后的数据怎么样，都要发布这个新功能，这时你要做的是如何优化用户体验
  - 比如，一些功能是法律规定的、或者属于公司策略性的，这些功能无论如何都要发布
- 样本数量太少，不能通过 A/B 测试得出合理、科学的结论
  - 比如，测试某个按钮的颜色设置为红色、绿色、蓝色，还是紫色的效果好，那么进行 A/B 测试时，你就需要至少4组对比试验，而且需要确保每一组试验都有足够的样本数量保证对比结果具有统计显著性
  - 如果这个按钮一共才20个人用，每组只有5个用户，那么得出的实验结果必然带有很大的偶然性，你无法根据这个实验数据做出科学的结论
  - 概要: 必须增加样本数量

#### 什么样的实验样本规模，才进行 A/B 测试呢？

- 取决于产品实验组和对比组之间的区别到底有多大
- 比如，Facebook 的朋友圈（News Feed），它是世界上最大的信息流产品，如果增加一个新功能可以让日活数增加1%，那这个功能就是巨大的成功。这是A/B 测试需要的样本数量就非常大，才能保证1%的进步具有统计显著性而不是误差
- 产品思路还没有定型的创业公司，有时一个新功能的发布可以将产品指标提升100%。即使没有那么多的样本数量，你可以肯定这个新功能可以给产品指标带来质的飞跃



### 短期数据 vs 长期数据

如果微信的摇一摇突然出现在了开启页面，那用户的使用量会直线上升（**直接得出摇一摇在微信开启页面的效果会更高的结论是不可信的**）

设置实验组的摇一摇在微信开启页面，而对照组的保留在发现页面，短期内肯定是实验组的数据更好看

没有意识到新功能的短期新鲜感和长期的生态系统影响，用户正在劲头上，好看的数据可以延续么？

建议

- 判断一个功能是不是值得发布时，应该等至少一个星期、短期的新鲜感褪去后，再衡量是否值得发布
- 如果通过 A/B 测试的结果决定要发布这个产品，应该留一个长期的对比实验组，比如1%的用户无法使用新产品，来观察这个产品对整个生态系统产生的影响，并适时作出调整

比如，Instagram 增加一个给好友点‘超级赞’的功能，目的是提高用户分享的频率

- 刚开始用户的活跃程度确定提高了，因为有了超级赞，发新鲜事倍有精神，分享数量大幅度提升，短期数据棒极了
- 长期来看，用户会因为只是得到了赞而没有获得超级赞，而感觉自尊心受损，最终不愿意也不敢分享，数据下降了



### 总结

首先，你要弄清楚要测的是什么，并且要100%清楚可以用什么指标衡量两组实验的区别

其次，验证因果性的唯一途径是 A/B 测试

再次，要判断是不是需要 A/B 测试，如果每一组的样本数量不够，或者功能无论如果都要发布，那么 A/B 测试并不是最好的方式

最后，A/B 测试需要考虑短期新鲜感和长期的生态系统影响。你可以保留一个长期的对比组，这样可以在产品发布之后也有较长的 A/B 测试时间，从而观察产品的长期表现，适时作出决定



### 思考题

你有没有经理过哪些产品（或者功能）的短期数据很好看，而长期数据却不好看？出现这个问题后，负责的产品经理是怎么处理的？







![silicon_valley_products](/img/silicon_valley_products/share.jpeg)









































